{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('hands_on_python_ml': conda)",
   "metadata": {
    "interpreter": {
     "hash": "daca65ddd2588cb6b7925fb08f88d3beb8117c35a7acc3efbcc17a025a6b580b"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# Pedestrian tracking with FairMOT object tracking model\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Overview\n",
    "This notebook implements a SOTA pedestrian tracking model, FairMOT, which integrate object detection and re-identification into a single deep neural network. Tasks include:\n",
    "\n",
    "1. Initiate a PyTorch version of DLA-34 baseline model, the backbone neural network of FairMOT. Load the pre-trained model weights that was trained on CrowdHuman and MIX datasets (see https://github.com/ifzhang/FairMOT for details).\n",
    "2. Load input video. For each frame:\n",
    " \n",
    " 2a. use the DLA-34 model to predict both object and object embeddings (i.e. features for identification).\n",
    " \n",
    " 2b. asssociate detected objects with already-tracked or new ID, by examing embeddings and distance moved across frames.\n",
    " \n",
    " 2c. generate output frame images with boxes and IDs\n",
    "\n",
    "3. Combine processed frames to create output video.\n",
    "4. (to be implemented) Export trajectories of the bottom center point of each bounding boxes, as the movement trajectories of people.\n",
    "\n",
    "Notes:\n",
    "- Tasks are carried out using open-source tool created by Yifu Zhang, which also contains scripts for model training and testing.\n",
    "- In this notebook, pre-trained model is used for demonstrative purpose.\n",
    "- In production, the FairMOT model will be trained on proper training data sets. Model checkpoints will be version-controlled, with approved ones saved to S3 for potential use. Model scripts will be containerized and stored in AWS ECR for use in AWS ECS or Fargate."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1) Install pacakges and setup environment\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install tools and packages\n",
    "!git clone https://github.com/ifzhang/FairMOT     # the FairMot scripts\n",
    "!git clone https://github.com/CharlesShang/DCNv2  # package for using DLA-34 model, the backbone neural network of FairMOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build DCNv2 \n",
    "!python ./DCNv2/setup.py build develop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# istall requirements of FairMOT\n",
    "!pip install -r ./external-repo/FairMOT/requirements.txt\n",
    "!conda install pytorch==1.2.0 torchvision==0.4.0 cudatoolkit=10.0 -c pytorch -y\n",
    "!conda install ffmpeg -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download pre-trained model weights of the DLA-34 backbone model.\n",
    "#  - url: https://drive.google.com/open?id=1udpOPum8fJdoEQm6n0jsIgMMViOMFinu\n",
    "#  - saved to S3 in advance\n",
    "import boto3\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# S3 bucket name and directories for the pre-trained DLA34 model\n",
    "bucket = 'pedestrian-tracker'  \n",
    "s3key_model = 'raw-pretrained-model'\n",
    "fname = 'fairmot_dla34.pth'\n",
    "s3.download_file(bucket, s3key + '/' + fname,  'external-repo/FairMOT/model/' + fname)"
   ]
  },
  {
   "source": [
    "## 2) Run FairMOT model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "# Execute FairMOT scripts with the following parameters\n",
    "#  - load_model: baseline PyTorch model check point to load \n",
    "#  - input-video: input video to analyze\n",
    "#  - conf_thres: Confidence threshold of tracking object identity; the highly, the more sceptical to assign an object to an ID\n",
    "#  - det_thres: Confidence threshold of detecting an object.\n",
    "#  - nmn_thres: Intersection over Union (IOU) threshold for non-max suppresion operation to remove bounding box proposals. Important if groups of people walk in close groups.\n",
    "#  - track_buffer: maximum number of video frames for which an object is allowed to be missing before considered 'lost'\n",
    "\n",
    "!python ./external-repo/FairMOT/src/demo.py mot --load_model ./external-repo/FairMOT/models/fairmot_dla34.pth \\\n",
    "        --conf_thres 0.3 --det_thres 0.3 --nms_thres 0.4 --track_buffer 30 \\\n",
    "        --input-video ./external-repo/FairMOT/videos/shopping-mall2.mp4 \\\n",
    "        --output-root ./external-repo/FairMOT/outputs\n"
   ]
  }
 ]
}
