{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('hands_on_python_ml': conda)",
   "metadata": {
    "interpreter": {
     "hash": "daca65ddd2588cb6b7925fb08f88d3beb8117c35a7acc3efbcc17a025a6b580b"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# Pedestrian tracking with FairMOT object tracking model\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Overview\n",
    "This notebook implements a SOTA pedestrian tracking model, FairMOT, which integrate object detection and re-identification into a single deep neural network. Tasks include:\n",
    "\n",
    "1. Initiate a PyTorch version of DLA-34 baseline model, the backbone neural network of FairMOT. Load the pre-trained model weights that was trained on CrowdHuman and MIX datasets (see https://github.com/ifzhang/FairMOT for details).\n",
    "2. Load input video. For each frame:\n",
    " \n",
    " 2a. use the DLA-34 model to predict both object and object embeddings (i.e. features for identification).\n",
    " \n",
    " 2b. asssociate detected objects with already-tracked or new ID, by examing embeddings and distance moved across frames.\n",
    " \n",
    " 2c. generate output frame images with boxes and IDs\n",
    "\n",
    "3. Combine processed frames to create output video.\n",
    "4. (to be implemented) Export trajectories of the bottom center point of each bounding boxes, as the movement trajectories of people.\n",
    "\n",
    "Notes:\n",
    "- Tasks are carried out using open-source tool created by Yifu Zhang, which also contains scripts for model training and testing.\n",
    "- In this notebook, pre-trained model is used for demonstrative purpose.\n",
    "- In production, the FairMOT model will be trained on proper training data sets. Model checkpoints will be version-controlled, with approved ones saved to S3 for potential use. Model scripts will be containerized and stored in AWS ECR for use in AWS ECS or Fargate."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1) Install pacakges and setup environment\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install tools and packages\n",
    "!git clone https://github.com/ifzhang/FairMOT     # the FairMot scripts\n",
    "!git clone https://github.com/CharlesShang/DCNv2  # package for using DLA-34 model, the backbone neural network of FairMOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build DCNv2 \n",
    "!python ./DCNv2/setup.py build develop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# istall requirements of FairMOT\n",
    "!pip install -r ./external-repo/FairMOT/requirements.txt\n",
    "!conda install pytorch==1.2.0 torchvision==0.4.0 cudatoolkit=10.0 -c pytorch -y\n",
    "!conda install ffmpeg -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download pre-trained model weights of the DLA-34 backbone model.\n",
    "#  - url: https://drive.google.com/open?id=1udpOPum8fJdoEQm6n0jsIgMMViOMFinu\n",
    "#  - saved to S3 in advance\n",
    "import boto3\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# S3 bucket name and directories for the pre-trained DLA34 model\n",
    "bucket = 'pedestrian-tracker'  \n",
    "s3key_model = 'raw-pretrained-model'\n",
    "fname = 'fairmot_dla34.pth'\n",
    "s3.download_file(bucket, s3key + '/' + fname,  'external-repo/FairMOT/model/' + fname)"
   ]
  },
  {
   "source": [
    "## 2) Run FairMOT model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute FairMOT scripts with the following parameters\n",
    "#  - load_model\n",
    "#  - input-video:\n",
    "#  - conf_thres: \n",
    "#  - det_thres:\n",
    "#  - nmn_thres:\n",
    "#  - track_buffer:\n",
    "\n",
    "!python ./external-repo/FairMOT/src/demo.py mot --load_model ./external-repo/FairMOT/models/fairmot_dla34.pth \\\n",
    "        --conf_thres 0.3 --det_thres 0.3 --nms_thres 0.4 --track_buffer 30 \\\n",
    "        --input-video ./external-repo/FairMOT/videos/shopping-mall2.mp4 \\\n",
    "        --output-root ./external-repo/FairMOT/outputs\n"
   ]
  }
 ]
}