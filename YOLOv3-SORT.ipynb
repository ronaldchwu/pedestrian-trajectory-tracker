{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('hands_on_python_ml': conda)",
   "metadata": {
    "interpreter": {
     "hash": "daca65ddd2588cb6b7925fb08f88d3beb8117c35a7acc3efbcc17a025a6b580b"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Pedestrian tracking with YOLOv3 object dection model and SORT tracking algorithm\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Overview\n",
    "This notebook implements a two-step (detect-then-track) pedestrian tracking service. Tasks include:\n",
    "- 1. Import input video and extract static frames\n",
    "- 2. For each frame, use YOLOv3 model to detect human objects, and send detected bounding boxes to an SORT (Simple online and realtime tracking) algorithm that tracks and associates each box with person ID.\n",
    "- 3. Combine processed frames to create output video with boxes and IDs.\n",
    "- 4. (to be implemented) Export trajectories of the bottom center point of each bounding boxes, as the movement trajectories of people.\n",
    "\n",
    "Notes:\n",
    "- In this notebook, a pretrained YOLOv3 model is deploymend on a AWS Sagemaker Endpoint for testing purpose. In production, trained YOLOv3 model will be included in a Docker image and making predictions within a AWS ECS or Fargate service."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1) Setup environment\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install tools and packages\n",
    "!conda install ffmpeg -y  # for generating output video from output frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, datetime, json, math, shutil, tarfile, random\n",
    "import os.path as osp\n",
    "import subprocess as sb\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import gluoncv\n",
    "from gluoncv import model_zoo, data, utils\n",
    "from gluoncv.utils.viz import plot_image\n",
    "import mxnet\n",
    "from mxnet import gluon, image, nd\n",
    "\n",
    "import boto3\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.mxnet.model import MXNetModel\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# S3 bucket name and directories for the YOLOv3 pre-trained model\n",
    "bucket = 'pedestrian-tracker'  \n",
    "s3key_model = 'detection-artifact'"
   ]
  },
  {
   "source": [
    "## 2) Import YOLOv3 model from gluoncv model zoo and save to S3\n",
    "- gluoncv is built upon the mxnet framework and provides SOTA deep learning algorithms in computer vision. \n",
    "- YOLOv3 model is imported, configurated, and saved to AWS S3 for later use."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the model to import\n",
    "model_name = 'yolo3_darknet53_coco'\n",
    "net = model_zoo.get_model(model_name, pretrained=True)\n",
    "\n",
    "# reset the detector to detect only the \"person\" class\n",
    "classes = ['person']\n",
    "net.reset_class(classes=classes, reuse_weights=classes)\n",
    "net.hybridize()  # switch to declarative execution to optimize computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the full model (both weights and graph)\n",
    "net.export(model_name, epoch=0)\n",
    "\n",
    "# compress\n",
    "packname = 'YOLOv3-darknet53-coco-model.tar.gz'\n",
    "tar = tarfile.open(packname, 'w:gz')\n",
    "tar.add('{}-symbol.json'.format(model_name))\n",
    "tar.add('{}-0000.params'.format(model_name))\n",
    "tar.close()\n",
    "\n",
    "# send to S3\n",
    "s3.upload_file(packname, bucket, s3key_model + '/' + packname)"
   ]
  },
  {
   "source": [
    "## 3) Deploy YOLOv3 model on a SageMaker endpoint\n",
    "Steps:\n",
    "-  1. Prepare requirements.txt and detection_server.py\n",
    "-  2. Instantiate model and deploy onto an endpoint"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare script in the 'repo' directory\n",
    "! mkdir repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile repo/requirements.txt\n",
    "# create a requirements.txt to add an extra dependency to the SageMaker MXNet container\n",
    "gluoncv==0.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile repo/detection_server.py\n",
    "# create a detection_server.py \n",
    "\n",
    "import argparse\n",
    "import ast\n",
    "import logging\n",
    "import os\n",
    "\n",
    "from gluoncv import model_zoo, data, utils\n",
    "import mxnet as mx\n",
    "from mxnet import nd, gluon\n",
    "\n",
    "def get_ctx():\n",
    "    \"function to get machine hardware context\"\n",
    "    try:\n",
    "        _ = mx.nd.array([0], ctx=mx.gpu())\n",
    "        ctx = mx.gpu()\n",
    "    except:\n",
    "        try:\n",
    "            _ = mx.nd.array([0], ctx=mx.eia())\n",
    "            ctx = mx.eia()\n",
    "        except: \n",
    "            ctx = mx.cpu()\n",
    "    return ctx\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"\n",
    "    Load the gluon model. Called once when hosting service starts.\n",
    "    :param: model_dir The directory where model files are stored.\n",
    "    :return: a model (in this case a Gluon network)\n",
    "    \n",
    "    assumes that the parameters artifact is {model_name}.params\n",
    "    \"\"\"\n",
    "    \n",
    "    ctx = get_ctx()\n",
    "    logging.info('Using ctx {}'.format(ctx))\n",
    "    logging.info('Dir content {}'.format(os.listdir()))\n",
    "    \n",
    "    # instantiate net and reset to classes of interest\n",
    "    net = gluon.nn.SymbolBlock.imports(\n",
    "        symbol_file=[f for f in os.listdir() if f.endswith('json')][0],\n",
    "        input_names=['data'],\n",
    "        param_file=[f for f in os.listdir() if f.endswith('params')][0],\n",
    "        ctx=ctx)\n",
    "    \n",
    "    return net\n",
    "\n",
    "\n",
    "def input_fn(request_body, request_content_type):\n",
    "    \"\"\"prepares the input\"\"\"\n",
    "        \n",
    "    im_array = mx.image.imdecode(request_body)\n",
    "    \n",
    "    # Run YOLO pre-processing on CPU\n",
    "    x, _ = data.transforms.presets.yolo.transform_test(im_array)\n",
    "    logging.info('input_fn returns NDArray of shape ' + str(im_array.shape))\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "def predict_fn(input_object, model):\n",
    "    \"\"\"function used for prediction\"\"\"\n",
    "    \n",
    "    ctx = get_ctx()\n",
    "    logging.info('Using ctx {}'.format(ctx))\n",
    "    \n",
    "    # forward pass and display\n",
    "    box_ids, scores, bboxes = model(input_object.as_in_context(ctx))\n",
    "    \n",
    "    return nd.concat(box_ids, scores, bboxes, dim=2)  # return a single tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model\n",
    "model = MXNetModel(\n",
    "    model_data='s3://{}/{}/{}'.format(bucket, s3key, packname),\n",
    "    role=get_execution_role(),\n",
    "    py_version='py3',\n",
    "    entry_point='detection_server.py',\n",
    "    source_dir='repo',\n",
    "    framework_version='1.6.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the name of the SageMaker endpoint, and deploy\n",
    "# - specify the EC2 instance type\n",
    "endpoint_key = ((model_name + '-detection').replace('_', '-').replace('.', '') + '-' \n",
    "                + datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S'))\n",
    "\n",
    "# this may take 5 to 10min\n",
    "model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    endpoint_name=endpoint_key)"
   ]
  },
  {
   "source": [
    "## 4) Implement SORT object tracking algorithm\n",
    "Steps of SORT\n",
    "- 1. Capture the current video frame, call YOLOv3 model to detect objects.\n",
    "- 2. Send detected objects (bounding boxes and prediction scores) to an Sort model. The Sort model will compare the current box locations against that in the previous frame, and associate them with object IDs.\n",
    "- 3. Iterate through all video frames and get output frames with annotated boxes with IDs. Combine them into an output video." 
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download sort.py of the SORT algorithm from source\n",
    "!wget 'https://raw.githubusercontent.com/abewley/sort/master/sort.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download input video data from s3 to local disk\n",
    "#  - assume that raw input video file is stored in S3://{bukcet}/raw-data\n",
    "s3key_input_video = 'raw-data'\n",
    "video_fname = 'shopping-mall2'\n",
    "video_ftype = 'mp4'\n",
    "full_fname = '{}.{}'.format(video_fname, video_ftype)\n",
    "s3.download_file(bucket, s3key_input_video+ '/' + full_fname, 'video/'+ full_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a SageMaker predictor using the endpoint in service\n",
    "predictor = sagemaker.predictor.Predictor(\n",
    "    endpoint_name=endpoint_key,\n",
    "    content_type='image/jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for calling the prediction endpoint\n",
    "# - send a single video frame to endpoint and get object detection results\n",
    "def detect(pic, predictor, input_type):\n",
    "    \"\"\"elementary function to send a picture to a predictor\"\"\"\n",
    "    if input_type == 'url':\n",
    "        with open(pic, 'rb') as image:\n",
    "            f = image.read()\n",
    "    elif input_type == 'byte':\n",
    "        f = pic\n",
    "    tensor = nd.array(json.loads(predictor.predict(f)))\n",
    "    box_ids, scores, bboxes = tensor[:,:,0], tensor[:,:,1], tensor[:,:,2:]\n",
    "    return box_ids, scores, bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videopath = 'video/'+ full_fname\n",
    "%pylab inline \n",
    "\n",
    "cmap = plt.get_cmap('tab20b')\n",
    "colors = [cmap(i)[:3] for i in np.linspace(0, 1, 20)]\n",
    "\n",
    "# get video parameters\n",
    "vid = cv2.VideoCapture(videopath) \n",
    "vid_width = vid.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "vid_height = vid.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "vid_FPS = vid.get(cv2.cv2.CAP_PROP_FPS)\n",
    "vid_n_frame = int(vid.get(cv2.cv2.CAP_PROP_FRAME_COUNT))\n",
    "vid_pos_frame = vid.get(cv2.cv2.CAP_PROP_POS_FRAMES)\n",
    "\n",
    "# initialize Sort object and set hyperparameters\n",
    "from sort import *\n",
    "mot_tracker = Sort(max_age = 20,  # Maximum number of frames to keep alive a track without associated detections\n",
    "                   min_hits = 3,  # Minimum number of associated detections before track is initialised.\n",
    "                   iou_threshold = 0.4)  # Minimum intersection-over-union (IOU) for calling a match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over individual video frames, update Sort object for object tracking\n",
    "import io\n",
    "\n",
    "# set prediction score threshold below which a bounding box is removed\n",
    "viz_tracking_prob_threshold = 0.75\n",
    "\n",
    "\n",
    "for ii in range(vid_n_frame):\n",
    "    ret, frame = vid.read()\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) # get current frame as an image\n",
    "    pilimg = Image.fromarray(frame)\n",
    "    temp = io.BytesIO() \n",
    "    pilimg.save(temp, format='PNG')\n",
    "    pilimg.save('temp/temp.png', format='PNG') # save the current video frame\n",
    "    img_byte_arr = temp.getvalue()\n",
    "\n",
    "    # run object detection on current frame image, by calling the Sagemaker endpoint\n",
    "    detections = detect(img_byte_arr, predictor, input_type='byte')\n",
    "\n",
    "    # get image frame for later adding tracking results\n",
    "    _, orig_img = data.transforms.presets.yolo.load_test('temp/temp.png')\n",
    "\n",
    "    # Extract bounding boxes and prediction scores\n",
    "    #  - remove boxes that have lower prediction scores than the specified threshold\n",
    "    idx_positive = np.argwhere(detections[1][0].asnumpy() > viz_tracking_prob_threshold).flatten()\n",
    "    bboxes = detections[2][0][idx_positive].asnumpy()\n",
    "    scores = detections[1][0][idx_positive].asnumpy().reshape(len(idx_positive),1)\n",
    "    det = numpy.concatenate([bboxes,scores],axis=1)\n",
    "    \n",
    "    # Send the location of bounding boxes and their prediction scores to Sort tracker\n",
    "    tracked_objects = mot_tracker.update(det)\n",
    "\n",
    "    # Plot and save the tracking outputs\n",
    "    # - plotting parameters\n",
    "    labels=None\n",
    "    class_names = classes\n",
    "    linewidth=1.5\n",
    "    fontsize=10\n",
    "\n",
    "    # - add bounding boxes and object ID to the current frame image\n",
    "    ax = plot_image(orig_img, ax=None, reverse_rgb=None);\n",
    "    if len(idx_positive) > 0:\n",
    "        # use random colors if None is provided\n",
    "        if colors is None:  \n",
    "            colors = dict()\n",
    "        for i, bbox_and_id in enumerate(tracked_objects):\n",
    "            cls_id = int(labels.flat[i]) if labels is not None else -1\n",
    "            if cls_id not in colors:\n",
    "                if class_names is not None:\n",
    "                    colors[cls_id] = plt.get_cmap('hsv')(cls_id / len(class_names))\n",
    "                else:\n",
    "                    colors[cls_id] = (random.random(), random.random(), random.random())\n",
    "            xmin, ymin, xmax, ymax = [int(x) for x in bbox_and_id[0:4]]\n",
    "            rect = plt.Rectangle((xmin, ymin), xmax - xmin,\n",
    "                                 ymax - ymin, fill=False,\n",
    "                                 edgecolor=colors[cls_id],\n",
    "                                 linewidth=linewidth)\n",
    "            ax.add_patch(rect);\n",
    "            if class_names is not None and cls_id < len(class_names):\n",
    "                class_name = class_names[cls_id]\n",
    "            else:\n",
    "                class_name = str(cls_id) if cls_id >= 0 else ''\n",
    "            objid = int(bbox_and_id[4])\n",
    "            if class_name or score:\n",
    "                ax.text(xmin, ymin - 2,\n",
    "                        '{}'.format(objid),\n",
    "                        #bbox=dict(facecolor=colors[cls_id], alpha=0.5),\n",
    "                        fontsize=fontsize, color=colors[cls_id]);\n",
    "    \n",
    "    # Save the annotated frame image\n",
    "    output_folder = 'outputs/SORT-{}'.format(video_fname)\n",
    "    fname = 'SORT-{}-frame-{}.jpg'.format(video_fname,str(ii).zfill(5))\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_folder, fname), dpi=200, bbox_inches='tight', pad_inches=0.0)\n",
    "    plt.close()\n",
    "    print('Frame number {} is processed.'.format(str(ii)))\n",
    "\n",
    "vid.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine annotated frames into an output video\n",
    "#  - use ffmpeg\n",
    "output_fname = 'SORT-{}-results.mp4'.format(video_fname)\n",
    "output_video_path = osp.join('outputs/videos', output_fname)\n",
    "cmd_str = 'ffmpeg -f image2 -i {}-%05d.jpg -b 5000k -c:v mpeg4 {}'.format(output_folder + '/SORT-' + video_fname + '-frame', output_video_path)\n",
    "os.system(cmd_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload output video to s3\n",
    "s3key_outputs = 'outputs'\n",
    "s3.upload_file(output_video_path, bucket, s3key_outputs + '/' + output_fname)"
   ]
  }
 ]
}
